import requests
from datetime import datetime, timedelta
from typing import List, Dict
import re

class NewsCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    
    def __init__(self, client_id=None, client_secret=None):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
    
    def search_news(self, keyword, display=20):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰. keywordê°€ ë¦¬ìŠ¤íŠ¸ì´ë©´ AND ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§."""
        if isinstance(keyword, list):
            # AND í‚¤ì›Œë“œ: ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ í›„, ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë°˜í™˜
            search_term = keyword[0]
            required_terms = keyword
        else:
            search_term = keyword
            required_terms = None

        if self.client_id and self.client_secret:
            articles = self._search_with_api(search_term, display)
        else:
            articles = self._search_with_crawl(search_term, display)

        if required_terms:
            articles = self._filter_by_all_terms(articles, required_terms)

        return articles

    def _filter_by_all_terms(self, articles, terms):
        """ê¸°ì‚¬ ì œëª©+ë³¸ë¬¸ì— ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²ƒë§Œ ë°˜í™˜"""
        filtered = []
        for article in articles:
            text = (article.get('title', '') + ' ' + article.get('description', '')).lower()
            if all(term.lower() in text for term in terms):
                filtered.append(article)
        print(f"ğŸ” AND í•„í„° ({' + '.join(terms)}): {len(articles)}ê°œ â†’ {len(filtered)}ê°œ")
        return filtered
    
    def _search_with_api(self, keyword, display):
        """ë„¤ì´ë²„ APIë¡œ ê²€ìƒ‰"""
        try:
            headers = {
                'X-Naver-Client-Id': self.client_id,
                'X-Naver-Client-Secret': self.client_secret
            }
            
            params = {
                'query': keyword,
                'display': min(display, 100),
                'sort': 'date'
            }
            
            response = requests.get(self.base_url, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            for item in data.get('items', []):
                article = {
                    'title': self._clean_html(item['title']),
                    'description': self._clean_html(item['description']),
                    'link': item.get('originallink') or item['link'],
                    'pubDate': self._parse_date(item['pubDate']),
                    'source': 'ë„¤ì´ë²„ ë‰´ìŠ¤'
                }
                articles.append(article)
            
            print(f"ğŸ“° ë„¤ì´ë²„ API: {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            return articles
            
        except Exception as e:
            print(f"âš ï¸  ë„¤ì´ë²„ API ì‹¤íŒ¨: {e}")
            return self._search_with_crawl(keyword, display)
    
    def _search_with_crawl(self, keyword, display):
        """ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ê²€ìƒ‰"""
        try:
            from bs4 import BeautifulSoup
            
            url = "https://search.naver.com/search.naver"
            params = {
                'where': 'news',
                'query': keyword,
                'sort': 0,
                'start': 1
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            news_items = soup.select('.news_area')
            
            for item in news_items[:display]:
                try:
                    title_elem = item.select_one('.news_tit')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    desc_elem = item.select_one('.news_dsc')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    info_elem = item.select_one('.info')
                    source = info_elem.get_text(strip=True) if info_elem else 'ë‰´ìŠ¤'
                    
                    article = {
                        'title': title,
                        'description': description,
                        'link': link,
                        'pubDate': datetime.now(),
                        'source': source
                    }
                    articles.append(article)
                    
                except Exception:
                    continue
            
            print(f"ğŸ“° ì›¹ í¬ë¡¤ë§: {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            return articles
            
        except Exception as e:
            print(f"âŒ ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def filter_recent(self, articles, hours=1):
        """ìµœê·¼ Nì‹œê°„ ë‚´ ê¸°ì‚¬ë§Œ í•„í„°ë§"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent = []
        for article in articles:
            pub_date = article.get('pubDate')
            if pub_date and pub_date.replace(tzinfo=None) >= cutoff:
                recent.append(article)
        print(f"â° ìµœê·¼ {hours}ì‹œê°„ í•„í„°: {len(articles)}ê°œ â†’ {len(recent)}ê°œ")
        return recent
    
    def _clean_html(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        text = re.sub('<[^<]+?>', '', text)
        text = text.replace('&quot;', '"')
        text = text.replace('&apos;', "'")
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        return text.strip()
    
    def _parse_date(self, date_str):
        """ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except:
            return datetime.now()
